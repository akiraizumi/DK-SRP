{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------------------------------------------------------------------------------------------------\n",
    "# プログラミング(Python) e-Learning用教材 演習編\n",
    "## 社会変革型 ライフサイエンス・ヘルスケア データサイエンティスト育成講座\n",
    "### Ver0.2(α版) 2019/1/11 Akira Izumi\n",
    "### ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ピマインディアンの糖尿病データ\n",
    "https://github.com/niharikagulati/diabetesprediction/blob/master/diabetes.csv  \n",
    "読み込んだデータの変数の意味については以下を参照\n",
    "\n",
    "| column # | variables |\n",
    "|:---------|:----------|\n",
    "| 0 | Number of times pregnant |\n",
    "| 1 | Plasma glucose concentration a 2 hours in an oral glucose tolerance test [mg/dL] |\n",
    "| 2 | Diastolic blood pressure [mm/Hg] |\n",
    "| 3 | Triceps skins fold thickness [mm] |\n",
    "| 4 | 2-hour serum insulin [mu U/ml] |\n",
    "| 5 | Body Mass Index |\n",
    "| 6 | Diabetes pedigree function |\n",
    "| 7 | Age [years] |\n",
    "| 8 | Outcome:  Class Variable {0, 1} where '1' denotes patient having diabetes | \n",
    "\n",
    "###  注意: データに欠損値があるため、null値をmode（最頻値）やmean（平均値）で置換することを推奨されています\n",
    "> Data Cleaning will take place as data has got lot of missing values.  \n",
    "> Handling missing values can be done either by replacing null values with mode or mean or replacing the null value with a random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Jupyter Notebookのホームディレクトリに上記csvファイルを格納してください\n",
    "\n",
    "# ファイル名\n",
    "filename = \"pima-indians-diabetes.data.csv\"\n",
    "\n",
    "# Pandasのread_csvメソッドを使用し、csvファイルを読み込みます\n",
    "# オプションのsepは区切り文字を指定します。\n",
    "# また、読み込むcsvファイルに列名がないため、namesオプションで列名を付与します\n",
    "df = pd.read_csv(filename, sep=',',\n",
    "                 names=[\n",
    "                     'Pregnancies',\n",
    "                     'Glucose',\n",
    "                     'Blood Pressure',\n",
    "                     'Skin Thickness',\n",
    "                     'Insulin',\n",
    "                     'BMI',\n",
    "                     'Diabetes Pedigree Function',\n",
    "                     'Age',\n",
    "                     'Outcome'\n",
    "                 ]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ファイルのパスを指定してもデータを読み込めます\n",
    "Linux/Unixであれば問題ありませんが、Windowsのパスを入れる場合は以下の対応が必要になります。\n",
    "- 文字列の前に\"r\"を入れる  \n",
    "例: filepath = r'C:\\Users\\ntohmatsu\\python_data\\pima-indians-diabetes.csv'\n",
    "- 区切り文字の円マークを1文字だけでなく、2文字入れる  \n",
    "例: filepath = 'C:\\\\Users\\\\ntohmatsu\\\\python_data\\\\pima-indians-diabetes.csv'\n",
    "\n",
    "df = pd.read_csv(filepath, sep=',',\n",
    "                 names=[\n",
    "                     'Pregnancies',\n",
    "                     'Glucose',\n",
    "                     'Blood Pressure',\n",
    "                     'Skin Thickness',\n",
    "                     'Insulin',\n",
    "                     'BMI',\n",
    "                     'Diabetes Pedigree Function',\n",
    "                     'Age',\n",
    "                     'Outcome'\n",
    "                 ]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 読み込んだデータの確認\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行数・列数の確認\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各変数のデータ型の確認\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの要約・可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各列にNullが何個あるか確認します。\n",
    "\n",
    "# メソッドisnullは、各要素が欠損値(NaN)であればTrue, そうでなければFalseを出力します\n",
    "# メソッドsumは、値の合計を出力します。オプションのaxis=0で行の合計、axis=1で列の合計を出力します\n",
    "# また、Trueは1、Falseは0と等価であることに注意してください\n",
    "\n",
    "# 結果から、欠損値はNaN以外で表現されていることが分かります\n",
    "\n",
    "print(df.isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本統計量の算出\n",
    "\n",
    "# データの個数、平均、楊淳偏差、最小値、データを昇順ソートしたときに25%, 50%, 75%目の値、最大値\n",
    "\n",
    "\"\"\"\n",
    "Pregnancies（妊娠回数）が'0'というのは違和感ありませんが、\n",
    "Glucose（血糖濃度）やBlood Pressure（血圧）が0というのは違和感があります\n",
    "そのため、欠損値は0として入力されていることが分かります\n",
    "\"\"\"\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# グループごとの基本統計量の算出\n",
    "\n",
    "# 糖尿病患者は、非患者と比較して、各変数の値が高い傾向にありそうです\n",
    "df_group_describe = df.groupby('Outcome').describe()\n",
    "for col_ind in df.columns[:-1]:\n",
    "    print(col_ind + \":\\n\" + str(df_group_describe[col_ind].T) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# データの可視化用ライブラリmatplotlibの利用\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# GlucoseやAgeに差がありそうです\n",
    "sns.pairplot(df, hue='Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 各データ間の相関を確認\n",
    "\n",
    "# 画像サイズの設定\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "# 相関係数行列のヒートマップ\n",
    "# AgeとPregnanciesに比較的高い相関がある等、確認できます\n",
    "sns.heatmap(df.corr(), cmap='bwr', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欠損値補完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値の個数を確認\n",
    "\n",
    "# 0と入力されて不適切な列を列挙\n",
    "imputer_cols = df.columns[[1,2,3,4,5,6]]\n",
    "\n",
    "# 各列において、0の個数を確認します\n",
    "df[imputer_cols][df[imputer_cols]==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 欠損値補完をするため、修正後のデータフレームcorrect_dfを作成します\n",
    "# 一度、dfと同じデータを読み込みます\n",
    "correct_df = df.copy()\n",
    "\n",
    "# 今回は、欠損値0の補完には\"算術平均値\"（meanメソッド）を用います\n",
    "# その他、\"中央値\"（medianアトリビュート）や\"最頻値\"（modeメソッド）を利用する場合もあります\n",
    "# 値の置換にはmaskメソッドを利用し、第1引数は条件、第2引数は置換する値になります\n",
    "for i in imputer_cols:\n",
    "    correct_df[i] = correct_df[i].mask(df[i]==0, df[i].mean())\n",
    "\n",
    "# Insulin列の0～2行目を見ると、元々\"0\"だったものが\"79.799479\"に置換されていることが分かります\n",
    "correct_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値修正後の基本統計量の算出\n",
    "\n",
    "# データの個数、平均、楊淳偏差、最小値、データを昇順ソートしたときに25%, 50%, 75%目の値、最大値\n",
    "# 修正対象列の最小値を見ると、0より大きくなっており、欠損値が保管されていることが分かります\n",
    "correct_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値修正後のグループ毎基本統計量の算出\n",
    "\n",
    "correct_df_group_describe = correct_df.groupby('Outcome').describe()\n",
    "for col_ind in correct_df.columns[:-1]:\n",
    "    print(col_ind + \":\\n\" + str(correct_df_group_describe[col_ind].T) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# データの可視化用ライブラリmatplotlibの利用\n",
    "\n",
    "sns.pairplot(correct_df, hue='Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各データ間の相関を確認\n",
    "\n",
    "# 画像サイズの設定\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "# 相関係数行列のヒートマップ\n",
    "# AgeとPregnanciesに比較的高い相関がある等、確認できます\n",
    "sns.heatmap(correct_df.corr(), cmap='bwr', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習モデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 説明変数と目的変数の選択\n",
    "\n",
    "# 説明変数の選択\n",
    "X = correct_df.loc[:, correct_df.columns[:-1]]\n",
    "\n",
    "# 目的変数の選択\n",
    "y = correct_df.loc[:, 'Outcome']\n",
    "\n",
    "# 正しく分割されたことを確認\n",
    "print(\"X=\\n\" + str(X.head()) + '\\n')\n",
    "print(\"y=\\n\" + str(y.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データと評価用データの分割\n",
    "\n",
    "# scikit-learnライブラリに簡単にデータを分割してくれるmodel_selectionメソッドがあります\n",
    "from sklearn import model_selection\n",
    "\n",
    "# train_test_splitメソッドを使うことで、\n",
    "# test_size（下記の場合は33%、つまり学習用データは67%）に従ってデータが分割されます\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=1/5, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# データが正しく分割できているか確認\n",
    "\n",
    "print(\"学習用説明変数のサイズは{}です\".format(X_train.shape))\n",
    "print(\"学習用目的変数のサイズは{}です\".format(y_train.shape))\n",
    "print(\"評価用説明変数のサイズは{}です\".format(X_test.shape))\n",
    "print(\"評価用目的変数のサイズは{}です\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# データの標準化\n",
    "\n",
    "# scikit-learnライブラリに簡単に標準化してくれるStandardScalerクラスが含まれています\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# インスタンスの生成\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 学習用データの標準化\n",
    "# fit_transformメソッドは各列の平均と分散を用いて標準化します\n",
    "# DataConversionWarningができますが、int64型をfloat64型に変換するためであり、無視して問題ございません\n",
    "st_X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# 評価用データの標準化\n",
    "# transformメソッドは、最後にfit_transformを利用したときの平均と分散を用いて標準化します\n",
    "# こちらもWarningがでますが、先ほどと同様のものであるため、無視して問題ございません。\n",
    "st_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#標準化できているか確認\n",
    "pd.DataFrame(st_X_train, columns=X.columns).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ロジスティック回帰モデル\n",
    "\n",
    "# LogisticRegressionクラスのインポート\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# インスタンスを生成します\n",
    "# オプションのsolverは最適化手法、max_iterは最大反復回数を表します\n",
    "lr = LogisticRegression(solver='lbfgs', class_weight='balanced',\n",
    "                        max_iter=1e5, random_state=0)\n",
    "\n",
    "# 学習（パラメータの推定）\n",
    "lr.fit(st_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回帰係数の可視化\n",
    "\n",
    "# DataFrameに格納\n",
    "coefs = pd.DataFrame(\n",
    "    lr.coef_,\n",
    "    columns=X.columns\n",
    ")\n",
    "coefs['intercept'] = lr.intercept_\n",
    "\n",
    "# DataFrameの可視化\n",
    "coefs.T.plot(y=coefs.index, kind='bar', title='coefficients', grid=True, figsize=(12,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 予測値の算出\n",
    "\n",
    "# 既に学習したmodelインスタンスに、predictメソッドを使うことで予測値を出力します\n",
    "y_pred = lr.predict(st_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの精度評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 精度評価の算出（一部可視化）\n",
    "\n",
    "# 混合行列、classification_reportの確認\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "labels = [1,0]\n",
    "\n",
    "# classification_reportの出力\n",
    "print(classification_report(y_test, y_pred, labels=labels))\n",
    "\n",
    "# 混合行列を出力します\n",
    "# 医療検査で1を陽性、0を陰性とすると、左上は真陽性、右下は真陰性、\n",
    "# 右上は偽陽性、左下は偽陰性になります。\n",
    "conf_mtx = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "# ヒートマップとして可視化します\n",
    "# オプションのannotは、ヒートマップの各セルにおける該当数を表示します\n",
    "sns.heatmap(conf_mtx, annot=True, xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパーパラメータチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 予測モデルの選択\n",
    "lr_gs = LogisticRegression(class_weight='balanced', max_iter=1e5,\n",
    "                           random_state=0, dual=False)\n",
    "\n",
    "# ハイパーパラメータの調整範囲\n",
    "# 正則化指標'penalty': 'L1ノルム'か'L2ノルム'\n",
    "# 正則化の強度'C': 10の-5乗～5乗まで、0.5乗刻み\n",
    "import numpy as np\n",
    "tuning_params = [\n",
    "    {\n",
    "        'penalty': ['l1'],\n",
    "        'solver': ['saga'],\n",
    "        'C': 10.**np.arange(-5, 5.5, 0.5)\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs'],\n",
    "        'C': 10.**np.arange(-5, 5.5, 0.5)\n",
    "    }\n",
    "]\n",
    "\n",
    "# グリッドサーチのgspmsインスタンス生成\n",
    "# 予測モデル: lr_gs（線形サポートベクターマシン識別機）\n",
    "# 調整するパラメータ: tuning_params\n",
    "# 交差確認法における分割ブロック数: cv\n",
    "# スコアリング方法: f1値、マイクロ平均\n",
    "# 並列処理数: n_jobs（1なら並列処理なし、-1なら全てのプロセッサ使用）\n",
    "# テストデータのサンプル数に基づくスコアへの重み付け: iid（将来、このオプションは削除される）\n",
    "# 学習データのスコア出力: return_train_score（現状のdefaultは'warm'だが、将来このオプションはFalseになる）\n",
    "lr_gs = GridSearchCV(\n",
    "    lr_gs, tuning_params,\n",
    "    cv=10, scoring='f1_micro', n_jobs=1, \n",
    "    iid=False, return_train_score=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グリッドサーチの実施\n",
    "\n",
    "lr_gs.fit(st_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 交差検証の結果（cv_results_属性）をDataFrameとして表示\n",
    "# 平均精度を降順に並べ替え\n",
    "\n",
    "lr_gs_results = pd.DataFrame(lr_gs.cv_results_)\n",
    "lr_gs_results.sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lr_gs_resultsのサイズ: ' + str(lr_gs_results.shape) )\n",
    "print('最良平均精度: ' + str(lr_gs_results.loc[lr_gs_results.index[0], 'mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 最適パラメータの確認（best_estimator_属性）\n",
    "\n",
    "# 交差検証の結果、正則化指標'penalty'はL2ノルム'、\n",
    "# 正則化の強度'C'は10の-1.5乗の組み合わせが最適であるという結果得る\n",
    "\n",
    "lr_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判別超平面の係数の可視化\n",
    "\n",
    "# DataFrameに格納\n",
    "coefs = pd.DataFrame(\n",
    "    lr_gs.best_estimator_.coef_,\n",
    "    columns=df.columns[:-1]\n",
    ")\n",
    "coefs['intercept'] = lr_gs.best_estimator_.intercept_\n",
    "\n",
    "# DataFrameの可視化\n",
    "coefs.T.plot(y=coefs.index, kind='bar', title='coefficients', grid=True, figsize=(12,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測値の取得\n",
    "\n",
    "y_pred = lr_gs.predict(st_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 評価指標の出力\n",
    "\n",
    "# classification_reportの出力\n",
    "# 結果として、標準的なパラメータと精度はほぼ変わらない\n",
    "print(classification_report(y_test, y_pred, labels=labels))\n",
    "\n",
    "# 混同行列の可視化\n",
    "conf_mtx = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "sns.heatmap(conf_mtx, annot=True, xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 他モデル（線形サポートベクターマシン識別器）の適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測モデルの選択\n",
    "from sklearn.svm import LinearSVC\n",
    "lsvc_gs = LinearSVC(class_weight='balanced', tol=1e-3,\n",
    "                    max_iter=1e4, random_state=0)\n",
    "\n",
    "# ハイパーパラメータの調整範囲\n",
    "# 正則化指標'penalty': 'L1ノルム'か'L2ノルム'\n",
    "# 正則化の強度'C': 10の-5乗～5乗まで、0.5乗刻み\n",
    "# 損失関数'loss': 'ヒンジ損失'か'二乗ヒンジ損失'\n",
    "\n",
    "tuning_params = [\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'loss': ['hinge'],\n",
    "        'dual': [True],\n",
    "        'C': 10.**np.arange(-5, 5.5, 0.5)\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l1'],\n",
    "        'loss': ['squared_hinge'],\n",
    "        'dual': [False],\n",
    "        'C': 10.**np.arange(-5, 5.5, 0.5)\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'loss': ['squared_hinge'],\n",
    "        'dual': [True],\n",
    "        'C': 10.**np.arange(-5, 5.5, 0.5)\n",
    "    }\n",
    "]\n",
    "\n",
    "# グリッドサーチのgspmsインスタンス生成\n",
    "# 予測モデル: lsvc_gs（線形サポートベクターマシン識別機）\n",
    "# 調整するパラメータ: tuning_params\n",
    "# 交差確認法における分割ブロック数: cv\n",
    "# スコアリング方法: f1値、マイクロ平均\n",
    "# 並列処理数: n_jobs（1なら並列処理なし、-1なら全てのプロセッサ使用）\n",
    "# テストデータのサンプル数に基づくスコアへの重み付け: iid（将来、このオプションは削除される）\n",
    "# 学習データのスコア出力: return_train_score（現状のdefaultは'warm'だが、将来このオプションはFalseになる）\n",
    "gs_lsvc = GridSearchCV(\n",
    "    lsvc_gs, tuning_params,\n",
    "    cv=10, scoring='f1_micro', n_jobs=-1, \n",
    "    iid=False, return_train_score=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グリッドサーチの実施\n",
    "\n",
    "gs_lsvc.fit(st_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交差検証の結果（cv_results_属性）をDataFrameとして表示\n",
    "# 平均精度を降順に並べ替え\n",
    "\n",
    "lsvc_gs_results = pd.DataFrame(gs_lsvc.cv_results_)\n",
    "lsvc_gs_results.sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('gs_resultsのサイズ: ' + str(lsvc_gs_results.shape) )\n",
    "print('最良平均精度: ' + str(lsvc_gs_results.loc[lsvc_gs_results.index[0], 'mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 最適パラメータの確認（best_estimator_属性）\n",
    "\n",
    "# 交差検証の結果、正則化指標'penalty'はL2ノルム'、\n",
    "# 正則化の強度'C'は10の1.5乗、損失関数'loss'は'ヒンジ損失'\n",
    "# の組み合わせが最適であるという結果を得る\n",
    "\n",
    "gs_lsvc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判別超平面の係数の可視化\n",
    "\n",
    "# DataFrameに格納\n",
    "coefs = pd.DataFrame(\n",
    "    gs_lsvc.best_estimator_.coef_,\n",
    "    columns=df.columns[:-1]\n",
    ")\n",
    "coefs['intercept'] = gs_lsvc.best_estimator_.intercept_\n",
    "\n",
    "# DataFrameの可視化\n",
    "coefs.T.plot(y=coefs.index, kind='bar', title='coefficients', grid=True, figsize=(12,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測値の取得\n",
    "\n",
    "y_pred = gs_lsvc.predict(st_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 評価指標の出力\n",
    "\n",
    "# classification_reportの出力\n",
    "# 結果として、グリッドサーチの最良平均精度は、ロジスティック回帰モデルのものよりも高いが、\n",
    "# 予測精度としては若干高い程度であった\n",
    "print(classification_report(y_test, y_pred, labels=labels))\n",
    "\n",
    "# 混同行列の可視化\n",
    "conf_mtx = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "sns.heatmap(conf_mtx, annot=True, xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 補足"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "今回利用した主なライブラリの説明は以下の通りです  \n",
    "http://www.numpy.org/  \n",
    "https://pandas.pydata.org/  \n",
    "https://matplotlib.org/  \n",
    "https://seaborn.pydata.org/  \n",
    "http://scikit-learn.org/stable/  \n",
    "\n",
    "今回利用したデータは以下からダウンロード可能  \n",
    "https://github.com/niharikagulati/diabetesprediction/blob/master/diabetes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の分析には、異常値の除外や非線形予測モデルの適用はしておりません。  \n",
    "これらの対応によって精度が向上するか、検証してみてください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
